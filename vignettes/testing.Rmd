---
title: "test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
require(simcausal)
library(sl3)
```

```{r}
passes<-c()
 for(i in 1:200){
   tmax <- 5
  print(i)
D <- DAG.empty()
D <- D + node("W1", distr = "runif",  min = -1, max = 1) + 
  node("Wa2", distr = "rbinom", size = 1, prob = 0.5) + 
   node("W2", distr = "rconst", const =Wa2 - 0.5 ) + 
   node("A", distr = "rbinom",  size = 1, prob = plogis(W1 + W2 )) +
  node("dNt", t = 1:tmax, EFU = TRUE , distr = "rbinom",  size = 1, prob = exp(0.5*A)*0.15*plogis(W1 + W2 )) +
  node("dCt", t = 1:tmax, EFU = TRUE,  distr = "rbinom",  size = 1, prob = 0*plogis(W1 + W2 + t)) 
D <- set.DAG(D)
data <- sim(D, n = 1000)
data
 
data_N <- data[, grep("[d][N].+", colnames(data)), drop = F]
data_C <- data[, grep("[d][C].+", colnames(data)), drop = F]

data_surv <-  as.data.frame(do.call(rbind, lapply(1:nrow(data), function(i) {
  rowN <- data_N[i,]
  rowC <- data_C[i,]
  t <- which(rowN==1)
  tc <- which(rowC==1)
  if(length(t)==0){
    t <- tmax+2
  }
  if(length(tc)==0){
    tc <- tmax + 1
  }
   
  Ttilde <- min(t,tc)
  Delta <- t <= tc
  return(matrix(c(Ttilde,Delta), nrow=1))
})))
colnames(data_surv) <- c("Ttilde", "Delta")
data$Ttilde <- data_surv$Ttilde
 data$Delta <- data_surv$Delta
 data <-  data[, -grep("[d][C].+", colnames(data))]
 data <-  data[, -grep("[d][N].+", colnames(data))]
  data
  
 

  doMC::registerDoMC(16)
  
  tmle_spec_np <- tmle3_Spec_coxph$new(formula = ~1,  delta_epsilon = 0.5, verbose = T, treatment_level = 1, control_level = 0)
  learner_list <- list(A = Lrnr_glm$new() , N = Lrnr_glm$new(formula = ~.^2), A_c =  Lrnr_glm$new(formula = ~.^2)) 
  node_list <- list(  W = c("W1", "W2"), A = "A", T_tilde = "Ttilde", Delta = "Delta" )
 
  tmle3_fit <- suppressMessages(suppressWarnings(tmle3(tmle_spec_np, data, node_list, learner_list)))
  
  print(tmle3_fit$summary)
  passes <- c(passes, tmle3_fit$summary$lower <= 0.5  & tmle3_fit$summary$upper >= 0.5 )
  print(mean(passes))
}


```
```{r}

tmle_task$get_regression_task("W")$Y

```
## R Markdown
```{r}


vet_data <- read.csv("https://raw.githubusercontent.com/tlverse/deming2019-workshop/master/data/veteran.csv")
vet_data$trt <- vet_data$trt - 1
vet_data$time <- ceiling(vet_data$time / 20)
node_list <- list(
  W = c("celltype" ), A = "trt", T_tilde = "time", Delta = "status",
  time = "t", N = "N", A_c = "A_c", id = "X", pre_failure = "pre_failure"
)

survival_spec <- tmle3_Spec_coxph$new(formula = ~ 1  ,
  treatment_level = 1, control_level = 0
)

learner_list <- list(A_c = Lrnr_xgboost$new(max_depth = 4), A = Lrnr_xgboost$new(max_depth = 4), N = Lrnr_xgboost$new(max_depth = 4))

tmle_task <- survival_spec$make_tmle_task(vet_data, node_list)
 
initial_likelihood <- survival_spec$make_initial_likelihood(tmle_task, learner_list)

up <- tmle3_Update$new(
  one_dimensional = T,
  maxit = 15,
  cvtmle = FALSE,
  constrain_step = T,
  convergence_type = "scaled_var",
  delta_epsilon = 0.05,
  use_best = F,
  verbose = TRUE
)

targeted_likelihood <- Targeted_Likelihood$new(initial_likelihood, updater = up)
tmle_params <- survival_spec$make_params(tmle_task, targeted_likelihood)

 
```


```{R}
tmle_fit_manual <- fit_tmle3(
  tmle_task, targeted_likelihood, tmle_params,
  targeted_likelihood$updater
)
tmle_fit_manual
```


```{r}
initial_likelihood$get_likelihoods(tmle_task)


```

```{r}
passes <- c()
passes1 <- c()
passes2 <- c()

for(i in 1:100){
  print(i)

n <- 1000
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(W))
Y <-  rnorm(n, mean =  A*W + A+W, sd = 0.3)
data <- data.table(W,A,Y)
lrnr_Y0W <- Lrnr_glmnet$new(formula = ~.^2)
lrnr_A <- Lrnr_glm$new()
 
node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W )
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1, "CATE")
out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list)
print("CATE")
```

```{r}
spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "TSM", submodel = "gaussian")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 
 out$tmle_params[[1]]$submodel
print("CATT")
 


spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "CATT")
 (out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
out <- out$summary
passes1 <- cbind(passes1 , out$lower <= 1 & out$upper >= 1)
 
print("CATE")
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1 + W, "CATE")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes2 <- cbind(passes2 , out$lower <= 1 & out$upper >= 1)
out
print(rowMeans(passes))
print(rowMeans(passes1))
print(rowMeans(passes2))
}

```


```{r}
n <- 500
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(W))
Y <-  rnorm(n, mean =  A*W + A+W, sd = 0.3) #rnorm(n, mean =  A*W + A+W, sd = 0.3)
data <- data.table(W,A,Y)
lrnr_Y0W <- Lrnr_gam$new()
lrnr_A <- Lrnr_gam$new()
 
node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W )

spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1  + W, "TSM" )
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes2 <- cbind(passes2 , out$lower <= 1 & out$upper >= 1)
out
```

 
 

```{r, include = T}

passes <- c()
passes1 <- c()
for(i in 1:100){
  print(i)
library(sl3)
n <- 750
W <- runif(n, -1, 1)
A <- rbinom(n, size = 1, prob = plogis(0))
Y <-  rbinom(n, size = 1, prob = plogis(A + W + A*W))
 quantile(plogis(1 + W) * (1-plogis(1 + W)) / ( plogis( W) * (1-plogis( W))))
data <- data.table(W,A,Y)
lrnr_Y0W <- Lrnr_earth$new()
lrnr_A <- Lrnr_earth$new()
node_list <- list (W = "W", A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W)
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1 + W, "OR")
 suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list))
 out <- out$summary
passes <- cbind(passes , out$lower <= 1 & out$upper >= 1)
print(out)

spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 + W, "OR")
suppressWarnings(out <- tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
passes1 <- cbind(passes1 , out$lower <= 1 & out$upper >= 1)
print(out)
print(rowMeans(passes))
print(rowMeans(passes1))
}
```

```{r}
#' library(glmnet)
#' n <- 200
#' W <- runif(n, -1, 1)
#' A <- rbinom(n, 1, plogis(W))
#' Y_continuous <- rnorm(n, mean =  A+W, sd = 0.3)
#' Y_binary <- rbinom(n, 1, plogis(A + W))
#' Y_count <- rpois(n, exp(A+W))
#' data <- data.table(W,A,Y_continuous, Y_binary, Y_count)

#' # Make tasks
#' task_continuous <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_continuous")
#' task_binary <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_binary")
#' task_count <- sl3_Task$new(data, covariates = c("A", "W"), outcome = "Y_count", outcome_type = "continuous")
#' 
#' formula_sp <- ~ 1 + W
#' 
# fit partially-linear least-squares regression with `append_interaction_matrix = TRUE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' V <- model.matrix(formula_sp, task_continuous$data)
#' X <- cbind(task_continuous$data[["W"]], task_continuous$data[["A"]]*V)
#' X0 <- cbind(task_continuous$data[["W"]], 0*V)
#' colnames(X) <- c("W", "A", "A*W")
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_equiv <- coef(cv.glmnet(X, Y, family = "gaussian"), s = "lambda.min")[c(3,4)] 
#' print(beta - beta_equiv) ## Actually, the glmnet fit is projected onto the semiparametric model using 
#' glm.fit. This has no effect on glmnet since it is linear. For nonlinear learners like Lrnr_gam or Lrnr_xgboost this projection has an effect.
#' 
#' # fit partially-linear least-squares regression with `append_interaction_matrix = FALSE`
#' set.seed(100)
#' lrnr_baseline <- Lrnr_glm$new(family = gaussian())
#' family <- gaussian()
#' lrnr_glm_sp_gaussian <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = FALSE)
#' lrnr_glm_sp_gaussian <- lrnr_glm_sp_gaussian$train(task_continuous)
#' preds <- lrnr_glm_sp_gaussian$predict(task_continuous)
#' beta <- lrnr_glm_sp_gaussian$fit_object$coefficients
#'  
## In this case, since `append_interaction_matrix = TRUE`, it is equivalent to:
#' subset_to <- task_continuous$data[["A"]]==0 # Subset to baseline treatment arm
#' V <- model.matrix(formula_sp, task_continuous$data)  
#' X <- cbind(rep(1,n), task_continuous$data[["W"]])   
#' Y <- task_continuous$Y
#' set.seed(100)
#' beta_Y0W <- lrnr_glm_sp_gaussian$fit_object$lrnr_baseline$fit_object$coefficients
#' beta_Y0W_equiv <- coef(glm.fit(X[subset_to,,drop=F],   Y[subset_to], family = gaussian()))  # Subset to baseline treatment arm
#' EY0 <- X %*% beta_Y0W
#' beta_equiv <- coef(glm.fit(A*V, Y, offset = EY0, family = gaussian())) 
#' print(beta_Y0W - beta_Y0W_equiv)
#' print(beta - beta_equiv)


 
#' # fit partially-linear logistic regression
#' lrnr_baseline <- Lrnr_glmnet$new()
#' family <- binomial()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_binary)
#' preds <- lrnr_glm_sp_binomial$predict(task_binary)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
 

#' # fit partially-linear log-link (elative-risk) regression
#' lrnr_baseline <- Lrnr_glmnet$new(family = "poisson") # This setting requires that lrnr_baseline predicts nonnegative values. It is recommended to use poisson regression based learners. 
#' family <- poisson()
#' lrnr_glm_sp_binomial <- Lrnr_glm_semiparametric$new(formula_sp = formula_sp, family = family, lrnr_baseline = lrnr_baseline, interaction_variable = "A", append_interaction_matrix = TRUE)
#' lrnr_glm_sp_binomial <- lrnr_glm_sp_binomial$train(task_count)
#' preds <- lrnr_glm_sp_binomial$predict(task_count)
#' beta <- lrnr_glm_sp_binomial$fit_object$coefficients
 

```





```{r}
library(sl3)
passes <- c()
passes1 <- c()
for(i in 1:100){
 
 
  print(i)
n <- 200
W <- as.matrix(replicate(2, runif(n, -1, 1)))
colnames(W) <- paste0("W", 1:ncol(W))
A <- rbinom(n, size = 1, prob = plogis(W[,1]))
Y <-  rpois(n, exp(A    + A*W[,1] + W[,1]))
data <- data.table(W,A,Y)
data
lrnr_Y0W <- Lrnr_glmnet$new(family = "poisson")
lrnr_A <- Lrnr_glmnet$new()

node_list <- list (W = colnames(W), A = "A", Y= "Y")
learner_list <- list(A  = lrnr_A, Y = lrnr_Y0W)
spec_spCATE <- tmle3_Spec_spCausalGLM$new(~1+W1 , "RR")
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
  out
passes <- cbind(passes , out$lower <= 1 & out$upper >= 1)

spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 , "RR", delta_epsilon = 0.01)
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
 out
 passes1 <- cbind(passes1 , out$lower <= 1 & out$upper >= 1)

 print(rowMeans(passes))
print(rowMeans(passes1))
}
 
```


```{r}
spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 , "RR", delta_epsilon = 0.01, family_fluctuation = "poisson")
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
 out

spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 , "RR", delta_epsilon = 10, family_fluctuation = "binomial")
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
 out

```


```{r}


out$tmle_task$npsem$Y$scale


spec_spCATE <- tmle3_Spec_npCausalGLM$new(~1 , "RR", delta_epsilon =0.001)
out <- suppressWarnings(tmle3(spec_spCATE, data, node_list, learner_list = learner_list) )
 out <- out$summary
 out


```


